import httpx
import pandas as pd
import re
import asyncio
from difflib import SequenceMatcher
import os

# API URLï¼ˆã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ç”¨ï¼‰
CROSSREF_API_URL = "https://api.crossref.org/works"
OPENALEX_API_URL = "https://api.openalex.org/works"

# ã—ãã„å€¤
SIMILARITY_THRESHOLD = 0.95

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°
async def fetch_metadata(title: str) -> dict:
    print("ğŸŸ©ğŸŸ©")
    crossref_task = asyncio.create_task(fetch_metadata_from_crossref(title, SIMILARITY_THRESHOLD))
    openalex_task = asyncio.create_task(fetch_metadata_from_openalex(title, SIMILARITY_THRESHOLD))
    print("ğŸŸ©ğŸŸ©")

    done, pending = await asyncio.wait(
        [crossref_task, openalex_task],
        return_when=asyncio.FIRST_COMPLETED
    )

    # Crossref ãŒå…ˆã«å®Œäº†ã—ãŸå ´åˆ
    if crossref_task in done:
        crossref_metadata = crossref_task.result()
        if "error" not in crossref_metadata:
            openalex_task.cancel()  # ä¸è¦ãªã®ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«
            metadata = crossref_metadata
            openalex = False
        else:
            # OpenAlex ã®å®Œäº†ã‚’å¾…ã¤
            openalex_metadata = await openalex_task
            if "error" not in openalex_metadata:
                metadata = openalex_metadata
                openalex = True
            else:
                metadata = {"error": "No data found in both Crossref and OpenAlex."}
                openalex = False

    # OpenAlex ãŒå…ˆã«å®Œäº†ã—ãŸå ´åˆ
    elif openalex_task in done:
        openalex_metadata = openalex_task.result()
        crossref_metadata = await crossref_task
        if "error" not in crossref_metadata:
            metadata = crossref_metadata
            openalex = False
        elif "error" not in openalex_metadata:
            metadata = openalex_metadata
            openalex = True
        else:
            metadata = {"error": "No data found in both Crossref and OpenAlex."}
            openalex = False

    # print(f"Metadata fetched: {metadata}")

    bibtex = None
    core_rank = "Unknown"
    acronym = "unknown"
    if not openalex:
        doi = metadata.get("doi")

        if doi:
            bibtex = await fetch_bibtex_from_doi(doi)
            # print(f"BibTeX fetched: {bibtex}")

            # bibtexã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ä¼šè­°ã‹ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«ã‹ã‚’åˆ¤å®š
            entry_type = extract_bibtex_type(bibtex)

            # ã‚³ã‚¢ãƒ©ãƒ³ã‚¯ã¨ä¼šè­°ç•¥ç§°ï¼ˆbibtexæ•´å½¢ç”¨ï¼‰ã‚’å–å¾—
            core_rank, acronym = get_core_rank_and_acronym(metadata.get("conference", ""), entry_type)

            # ãƒ©ãƒ™ãƒ«ã®æ›¸ãæ›ãˆ
            if metadata.get("authors") and metadata.get("year"):
                for author in metadata["authors"]:
                    if author != '':
                        bibtex = rewrite_bibtex_label(bibtex, metadata["authors"][0], metadata["year"], acronym)
                        if acronym != "unknown":
                            metadata["conference"] = acronym
                        break

    else:
        bibtex = fetch_bibtex_openalex(metadata)

    final_metadata = {
        "title": metadata.get("title"),
        "authors": metadata.get("authors"),
        "year": metadata.get("year"),
        "conference": metadata.get("conference"),
        "bibtex": bibtex,
        "citations": metadata.get("citations"),
        "core_rank": core_rank
    }

    return final_metadata, openalex


def preprocess(text: str) -> str:
    # è‹±æ•°å­—ã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã‚„è¨˜å·ã‚’å‰Šé™¤ï¼‰
    return re.sub(r'[^a-zA-Z0-9]', '', text.lower())

def preprocess_not_lower(text: str) -> str:
    # è‹±æ•°å­—ã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã‚„è¨˜å·ã‚’å‰Šé™¤ï¼‰
    return re.sub(r'[^a-zA-Z0-9]', '', text)

# é¡ä¼¼åº¦è¨ˆç®—ï¼ˆå®Œå…¨ä¸€è‡´ã«è¿‘ã„æ¯”è¼ƒï¼‰
def similarity(t1: str, t2: str) -> float:
    return SequenceMatcher(None, preprocess(t1), preprocess(t2)).ratio()

# OpenAlex fallback
async def fetch_metadata_from_openalex(title: str, similarity_threshold: float) -> dict:
    params = {
        "search": title,
        "per-page": 3
    }

    try:
        async with httpx.AsyncClient(timeout=30) as client:
            print("ğŸŸ¨ğŸŸ¨ğŸŸ¨")
            response = await client.get(OPENALEX_API_URL, params=params)
            print("ğŸŸ¨ğŸŸ¨ğŸŸ¨ğŸŸ¨")
        response.raise_for_status()
        data = response.json()
        results = data.get("results", [])

        if not results:
            return {"error": "No data found in OpenAlex."}

        best_item = None
        best_score = -1.0

        for item in results:
            item_title = item.get("title", "")
            # print(f"[OpenAlex] Evaluating item: {item_title}")
            score = similarity(title, item_title)
            # print(f"[OpenAlex] Similarity score for '{item_title}': {score}")

            if (
                score > best_score or
                (score == best_score and best_item and is_newer_openalex(item, best_item))
            ):
                best_item = item
                best_score = score

        if best_item and best_score >= similarity_threshold:
            return {
                "title": best_item.get("title"),
                "authors": [a.get("author", {}).get("display_name", "") for a in best_item.get("authorships", [])],
                "year": best_item.get("publication_year"),
                "conference": best_item.get("host_venue", {}).get("display_name"),
                "citations": best_item.get("cited_by_count"),
                "doi": best_item.get("doi")
            }
        else:
            return {"error": "No matching paper in OpenAlex."}

    except httpx.RequestError as e:
        return {"error": f"OpenAlex request failed: {e}"}
    
def is_newer_openalex(item1, item2):
    # "publication_date": "2023-05-15"
    date1 = item1.get("publication_date", "")
    date2 = item2.get("publication_date", "")
    return date1 > date2

async def fetch_metadata_from_crossref(title: str, similarity_threshold: float) -> dict:
    params = {
        "query.title": title,
        "rows": 3,
        "sort": "score"
    }

    headers = {
        "User-Agent": "MyResearchBot/1.0"
    }

    try:
        async with httpx.AsyncClient(timeout=30) as client:
            print("ğŸŸ©ğŸŸ©ğŸŸ©")
            response = await client.get(CROSSREF_API_URL, params=params, headers=headers)
            print("ğŸŸ©ğŸŸ©ğŸŸ©ğŸŸ©")
        response.raise_for_status()

        data = response.json()
        items = data.get("message", {}).get("items", [])

        if not items:
            return {"error": "No data found in Crossref."}

        best_item = None
        best_score = -1.0

        for item in items:
            item_title = item.get("title", [None])[0]
            if not item_title or item.get("type") == "posted-content":
                continue

            # print(f"[Crossref] Evaluating item: {item_title}")
            score = similarity(title, item_title)
            # print(f"[Crossref] Similarity score for '{item_title}': {score}")

            if (score > best_score or
                (score == best_score and best_item and is_newer_crossref(item, best_item))):
                best_item = item
                best_score = score

        if best_item and best_score >= similarity_threshold:
            return {
                "title": best_item.get("title", [None])[0],
                "authors": [f"{a.get('given', '')} {a.get('family', '')}".strip() for a in best_item.get("author", [])],
                "year": best_item.get("issued", {}).get("date-parts", [[None]])[0][0],
                "conference": best_item.get("container-title", [None])[0],
                "citations": best_item.get("is-referenced-by-count"),
                "doi": best_item.get("DOI")
            }
        else:
            return {"error": "No matching paper in Crossref."}

    except httpx.RequestError as e:
        return {"error": f"Request failed: {e}"}
    
# è«–æ–‡ç™ºè¡Œæ—¥ã®æ¯”è¼ƒé–¢æ•°
def is_newer_crossref(item1, item2):
    """Return True if item1 is more recent than item2 (compare year, month, day)."""
    date1 = item1.get("issued", {}).get("date-parts", [[0]])[0]
    date2 = item2.get("issued", {}).get("date-parts", [[0]])[0]

    # å¹´ãƒ»æœˆãƒ»æ—¥ã®3è¦ç´ ã«æƒãˆã‚‹ï¼ˆè¶³ã‚Šãªã„éƒ¨åˆ†ã¯0ã§åŸ‹ã‚ã‚‹ï¼‰
    date1_full = tuple(date1 + [0] * (3 - len(date1)))
    date2_full = tuple(date2 + [0] * (3 - len(date2)))

    return date1_full > date2_full

# DOIã‹ã‚‰BibTeXã‚’å–å¾—
async def fetch_bibtex_from_doi(doi: str) -> str:
    headers = {
        "User-Agent": "MyResearchBot/1.0"
    }

    url = f"https://api.crossref.org/works/{doi}/transform/application/x-bibtex"

    try:
        async with httpx.AsyncClient(timeout=10) as client:
            response = await client.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching BibTeX via DOI: {e}")
        return None

# BibTeXã‚¨ãƒ³ãƒˆãƒªã®ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡ºï¼ˆ@article, @inproceedingsãªã©ï¼‰
def extract_bibtex_type(bibtex: str) -> str:
    bibtex = bibtex.lstrip()  # å…ˆé ­ã®ç©ºç™½ãƒ»æ”¹è¡Œã‚’å‰Šé™¤
    match = re.match(r"@(\w+)\{", bibtex)
    if match:
        return match.group(1).lower()
    return ""

# ã‚³ã‚¢ãƒ©ãƒ³ã‚¯ã¨ä¼šè­°ç•¥ç§°ã‚’è¿”ã™é–¢æ•°
def get_core_rank_and_acronym(conference_name: str, entry_type: str) -> tuple[str, str]:
    base_dir = os.path.dirname(os.path.abspath(__file__))

    csv_path = os.path.join(
        base_dir,
        "conference_core_rank.csv" if entry_type == "inproceedings" else "journal_core_rank.csv"
    )

    try:
        df = pd.read_csv(csv_path)
        df = df.dropna(subset=['Title', 'Acronym'])

        normalized_conf_name = preprocess(conference_name)
        normalized_conf_name_not_lower = preprocess_not_lower(conference_name)

        for _, row in df.iterrows():
            title = preprocess(str(row['Title']))
            acronym = row['Acronym']

            if title in normalized_conf_name or acronym in normalized_conf_name_not_lower:
                return row.get('Rank', 'Unknown'), row.get('Acronym', 'unknown')

        return "Unknown", "unknown"

    except Exception as e:
        print(f"Error loading rank CSV: {e}")
        return "Error", "unknown"

# BibTeXãƒ©ãƒ™ãƒ«ã‚’ç­†é ­è‘—è€…+å¹´+ä¼šè­°ç•¥ç§°ã«æ›¸ãæ›ãˆã‚‹
def rewrite_bibtex_label(bibtex_str: str, author: str, year: int, acronym: str) -> str:
    if not bibtex_str or acronym == "unknown":
        return bibtex_str

    last_name = author.strip().split()[-1]
    new_label = f"{last_name}{year}{acronym}"

    # @type{oldlabel, ã‚’ @type{newlabel, ã«ç½®æ›
    return re.sub(r'(@\w+\{)[^,]+', r'\1' + new_label, bibtex_str, count=1)

def fetch_bibtex_openalex(metadata: dict) -> str:
    authors = metadata.get("authors", [])
    if authors:
        first_author = authors[0]
        first_author_surname = first_author.split()[-1].lower()
    else:
        first_author_surname = "anonymous"

    year = str(metadata.get("year") or "xxxx").strip()
    title = (metadata.get("title") or "").strip()
    conference = metadata.get("conference")
    doi = (metadata.get("doi") or "").strip()

    def format_author(name: str) -> str:
        parts = name.strip().split()
        if len(parts) >= 2:
            given = " ".join(parts[:-1])
            family = parts[-1]
            return f"{family}, {given}"
        return name.strip()

    formatted_authors = [format_author(a) for a in authors]
    bibtex_authors = " and ".join(formatted_authors)

    bibtex_id = f"{first_author_surname}{year}"

    bibtex_lines = [
        f"@article{{{bibtex_id}}},",
        f"  title={{{title}}},",
        f"  author={{{bibtex_authors}}},",
        f"  year={{{year}}},"
    ]

    if conference:
        bibtex_lines.append(f"  booktitle={{{conference.strip()}}},")

    if doi:
        bibtex_lines.append(f"  doi={{{doi}}}")

    bibtex_lines.append("}")

    return "\n".join(bibtex_lines)

# ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
if __name__ == "__main__":
    title = "Using Gameplay Videos for Detecting Issues in Video Games"
    # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    metadata = fetch_metadata(title)
    # print(f"\nFinal metadata with core rank:\n{metadata}")
