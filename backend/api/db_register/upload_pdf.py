# import os
import time
import hashlib
import uuid
import asyncio
from typing import Dict, Tuple

import fitz
from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile
from fastapi.responses import FileResponse
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.base import RunnableBinding
from langchain_core.vectorstores.base import VectorStoreRetriever
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from pypdf import PdfReader
from sqlalchemy.orm import Session

from backend.api.db_register.db_register import register_paper
from backend.api.db_register.get_paper_title import get_paper_title
from backend.api.db_register.metadata_fetcher import fetch_metadata
from backend.config.config import (
    BASE_URL,
    CHAT_MODEL,
    EMBEDDINGS_MODEL,
    GROQ_API_KEY,
    UPLOAD_DIR,
    VECTOR_STORE_DIR,
)
from backend.database.database import get_db
from backend.models.models import Paper, User
from backend.schema.schema import PaperSchema, UploadPDFResponseSchema
from backend.utils.security import get_current_user
from backend.utils.vector_store import get_vector_store
from backend.utils.tranalate import translate

router = APIRouter()  # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ

# os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Groqã®API keyã‚’å–å¾—
groq_chat = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=CHAT_MODEL)

system_prompt = "You are a helpful assistant. Please respond based on the content of the paper PDF.\n\n{context}"
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL)

# ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
# def load_vector_store() -> FAISS:
#     if os.path.exists(VECTOR_STORE_DIR):
#         vector_store = FAISS.load_local(
#             VECTOR_STORE_DIR, embeddings, allow_dangerous_deserialization=True
#         )
#     else:
#         documents = [
#             Document(
#                 page_content="",
#                 metadata={"paper_id": "", "user_id": "", "category": ""},
#             )
#         ]
#         vector_store = FAISS.from_documents(documents, embeddings)

#     return vector_store

# PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹é–¢æ•°
def save_pdf(pdf_bytes, copy_pdf_path):
    if not UPLOAD_DIR.exists():
        UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
    with open(copy_pdf_path, "wb") as f:
        f.write(pdf_bytes)

# PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
def read_text_from_pdf(pdf_path):
    reader = PdfReader(pdf_path)
    text = ""
    for page_num in range(len(reader.pages)):
        page = reader.pages[page_num]
        text += page.extract_text()
    return text


# PDFã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
def split_pdf_text(pdf_text: str) -> Tuple[list, int]:
    # ãƒãƒ£ãƒ³ã‚¯é–“ã§overlappingã•ã›ãªãŒã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=3000,
        chunk_overlap=50,
    )
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
    splited_text = text_splitter.split_text(pdf_text)
    chunk_count = len(splited_text)
    return splited_text, chunk_count


# ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
def embedding_text(
    splited_text: list, pdf_id: str, user_id: int, category: str
) -> FAISS:
    documents = [
        Document(
            page_content=text,
            metadata={"paper_id": pdf_id, "user_id": user_id, "category": category},
        )
        for text in splited_text
    ]

    doc_ids = [f"{pdf_id}_{i}" for i in range(len(splited_text))]
    index = FAISS.from_documents(documents, embedding=embeddings, ids=doc_ids)
    return index


# Faissã®Retrieverã‚’å–å¾—
def get_retriever(index: FAISS) -> VectorStoreRetriever:
    retriever = index.as_retriever(search_kwargs={"k": 6})
    return retriever


# RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ
def create_rag_chain(
    retriever: VectorStoreRetriever, chat: ChatGroq, prompt: ChatPromptTemplate
) -> RunnableBinding:
    question_answer_chain = create_stuff_documents_chain(chat, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)
    return rag_chain


# è«–æ–‡ã®è¦ç´„ã‚’ç”Ÿæˆ
def generate_summary(rag_chain: RunnableBinding) -> str:
    user_input = "Read all pages of the PDF and summarize the paper. The output format is as follows.\nSummary: {summary}"
    response = rag_chain.invoke({"input": user_input})
    return response["answer"]


# èµ·å‹•ã®ç¢ºèªç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@router.get("/")
def read_root():
    return {"message": "Hello, FastAPI is running!"}


# è«–æ–‡1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆ
def calculate_first_page_hash(pdf_bytes: bytes) -> str:
    """PDFã®1ãƒšãƒ¼ã‚¸ç›®ã®å†…å®¹ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆ"""
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    if len(doc) == 0:
        return None  # ç©ºPDF
    page = doc.load_page(0)  # 1ãƒšãƒ¼ã‚¸ç›®ï¼ˆ0-indexedï¼‰

    # ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’å–ã‚‹
    text = page.get_text()
    hash_value = hashlib.sha256(text.encode("utf-8")).hexdigest()
    return hash_value

async def prepare_metadata(title: str) -> Tuple[Dict[str, str], bool]:
    metadata_start = time.perf_counter()
    print("ğŸŸ©")
    openalex = False

    if title is not None:
        metadata, openalex = await fetch_metadata(title)
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¦ã„ãªã‹ã£ãŸå ´åˆï¼Œã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ 
        if "error" in metadata:
            metadata["title"] = title
    else:
        metadata = {
            "title": None,
            "authors": None,
            "year": None,
            "conference": None,
            "bibtex": None,
            "citations": None,
            "core_rank": None,
        }
    metadata_end = time.perf_counter()
    print(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚é–“: {metadata_end - metadata_start:.2f}ç§’")
    return metadata, openalex

async def prepare_summary(copy_pdf_path: str, pdf_id: str, user_id: int, category: str) -> Tuple[str, int]:
    summary_start = time.perf_counter()
    await asyncio.sleep(0.1)  # å°‘ã—å¾…ã¤

    print("ğŸŸ¥")
    pdf_text = read_text_from_pdf(str(copy_pdf_path))
    one_end = time.perf_counter()
    print(f"PDFèª­ã¿è¾¼ã¿æ™‚é–“: {one_end - summary_start:.2f}ç§’")
    splited_txt, chunk_count = split_pdf_text(pdf_text)
    two_end = time.perf_counter()
    print(f"ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²æ™‚é–“: {two_end - one_end:.2f}ç§’")
    index = embedding_text(splited_txt, pdf_id, user_id, category)
    three_end = time.perf_counter()
    print(f"åŸ‹ã‚è¾¼ã¿æ™‚é–“: {three_end - two_end:.2f}ç§’")
    # print(index.similarity_search("test", k=1))

    retriever = get_retriever(index)
    four_end = time.perf_counter()
    print(f"Retrieverå–å¾—æ™‚é–“: {four_end - three_end:.2f}ç§’")
    rag_chain = create_rag_chain(retriever, groq_chat, prompt)
    five_end = time.perf_counter()
    print(f"RAGãƒã‚§ãƒ¼ãƒ³ä½œæˆæ™‚é–“: {five_end - four_end:.2f}ç§’")
    print("ğŸŸ¥ğŸŸ¥")
    summary = await asyncio.to_thread(generate_summary, rag_chain)
    print("ğŸŸ¥ğŸŸ¥ğŸŸ¥")
    six_end = time.perf_counter()
    print(f"è¦ç´„ç”Ÿæˆæ™‚é–“: {six_end - five_end:.2f}ç§’")
    summary = summary.replace("Summary: ", "")
    seven_end = time.perf_counter()
    print(f"è¦ç´„æ•´å½¢æ™‚é–“: {seven_end - six_end:.2f}ç§’")
    summary = await asyncio.to_thread(translate, summary, "ja")
    print("ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥")
    eight_end = time.perf_counter()
    print(f"è¦ç´„ç¿»è¨³æ™‚é–“: {eight_end - seven_end:.2f}ç§’")

    vector_store = get_vector_store()
    print("ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥")
    nine_end = time.perf_counter()
    print(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å–å¾—æ™‚é–“: {nine_end - eight_end:.2f}ç§’")
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 

    vector_store.merge_from(index)
    ten_end = time.perf_counter()
    print(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒãƒ¼ã‚¸æ™‚é–“: {ten_end - nine_end:.2f}ç§’")
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜
    vector_store.save_local(VECTOR_STORE_DIR)
    summary_end = time.perf_counter()
    print(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜æ™‚é–“: {summary_end - ten_end:.2f}ç§’")
    print(f"è¦ç´„ç”Ÿæˆæ™‚é–“: {summary_end - summary_start:.2f}ç§’")

    return summary, chunk_count

# PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# FastAPIã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@router.post("/upload", response_model=UploadPDFResponseSchema)
async def upload_pdf(
    file: UploadFile = File(...),
    category: str = Form(None),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    start = time.monotonic()
    # PDFã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if not file.filename.endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Only PDF files are allowed.")
    pdf_bytes = await file.read()

    # PDFã®1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
    pdf_hash = calculate_first_page_hash(pdf_bytes)
    if pdf_hash is None:
        raise HTTPException(status_code=400, detail="PDFãŒç©ºã§ã™ï¼")

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼šåŒã˜ã‚«ãƒ†ã‚´ãƒªã¨ãƒãƒƒã‚·ãƒ¥ã®PDFãŒæ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ï¼Ÿ
    existing = (
        db.query(Paper)
        .filter_by(user_id=current_user.id, category=category, hash=pdf_hash)
        .first()
    )
    if existing:
        raise HTTPException(status_code=409, detail="ã“ã®PDFã¯ã™ã§ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ï¼")

    # PDFã‚’ä¿å­˜
    pdf_id = str(uuid.uuid4())
    pdf_url = f"{BASE_URL}/uploaded/{pdf_id}.pdf"

    copy_pdf_path = UPLOAD_DIR / f"{pdf_id}.pdf"

    save_pdf(pdf_bytes, copy_pdf_path)

    title = get_paper_title(copy_pdf_path)

    metadata_task = asyncio.create_task(prepare_metadata(title))
    summary_task = asyncio.create_task(prepare_summary(copy_pdf_path, pdf_id, current_user.id, category))

    # ä¸¦åˆ—ã«å®Ÿè¡Œ
    (summary, chunk_count), (metadata, openalex) = await asyncio.gather(
        summary_task,
        metadata_task
    )

    metadata.update(
        {
            "pdf_id": pdf_id,
            "pdf_url": pdf_url,
            "summary": summary,
            "category": category,
            "hash": pdf_hash,
            "user_id": current_user.id,
            "chunk_count": chunk_count,
        }
    )
    suc_or_fai = "failure"
    suc_or_fai = register_paper(metadata)
    final_data = PaperSchema(
        paper_id=metadata.get("pdf_id"),
        title=metadata.get("title"),
        authors=metadata.get("authors"),
        year=metadata.get("year"),
        conference=metadata.get("conference"),
        bibtex=metadata.get("bibtex"),
        citations=metadata.get("citations"),
        core_rank=metadata.get("core_rank"),
        pdf_url=metadata.get("pdf_url"),
        category=metadata.get("category"),
        summary=metadata.get("summary"),
        user_id=metadata.get("user_id"),
    )
    final_data = final_data.model_dump()

    if suc_or_fai == "failure":
        response = UploadPDFResponseSchema(
            success=False,
            message="ç™»éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼",
            data=final_data,
        )
    elif suc_or_fai == "success":
        # å€‹åˆ¥ã®é …ç›®ãŒå–å¾—ã§ãã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        missing_fields = []
        if metadata.get("title") is None:
            missing_fields.append("ã‚¿ã‚¤ãƒˆãƒ«")
        if metadata.get("authors") is None or not metadata["authors"]:
            missing_fields.append("è‘—è€…æƒ…å ±")
        if metadata.get("year") is None:
            missing_fields.append("å‡ºç‰ˆå¹´")
        if not openalex:
            if metadata.get("conference") is None:
                missing_fields.append("ä¼šè­°/ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å")
        if metadata.get("bibtex") is None:
            missing_fields.append("BibTeX")
        if metadata.get("citations") is None:
            missing_fields.append("è¢«å¼•ç”¨æ•°")
        if not openalex:
            if metadata.get("core_rank") in ["Unknown", "Not found", "Error"]:
                missing_fields.append("COREãƒ©ãƒ³ã‚¯")


        # å¤±æ•—é …ç›®ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’åˆ—æŒ™ã—ã¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        if missing_fields:
            failed_info = ", ".join(missing_fields)
            response = UploadPDFResponseSchema(
                success=False,
                message=f"{failed_info} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼",
                data=final_data,
            )
        else:
            response = UploadPDFResponseSchema(
                success=True,
                message="PDFã®ç™»éŒ²ãŒå®Œäº†ã—ã¾ã—ãŸï¼",
                data=final_data,
            )

    # print(response)
    end = time.monotonic()
    print(f"PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†æ™‚é–“: {end - start:.2f}ç§’")
    return response


# PDFã‚’é–‹ã
@router.get("/uploaded/{paper_id}.pdf")
async def get_pdf(paper_id: str):
    pdf_path = UPLOAD_DIR / f"{paper_id}.pdf"

    if not pdf_path.exists():
        raise HTTPException(status_code=404, detail="PDFãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼")

    return FileResponse(
        path=pdf_path,
        media_type="application/pdf",
        filename=f"{paper_id}.pdf",
        headers={"Content-Disposition": f"inline; filename={paper_id}.pdf"},
    )
